---
title: "Dynamic Sampling and Selective Masking for Communication-Efficient Federated Learning"
collection: publications
authors_before_me: "Shaoxiong Ji*, "
author_me: "Wenqi Jiang* (co-first author)"
authors_after_me: ", Anwar Walid, Xue Li"
permalink: /publication/2020-3-21-Dynamic-Sampling-and-Selective-Masking-for-Communication-Efficient-Federated-Learning
excerpt: ~
date: 2020-3-21
venue: 'arXiv preprint'
paperurl: ~ 
citation:  ~ 
---

<p></p>

[Paper](https://arxiv.org/pdf/2003.09603.pdf)

Federated learning (FL) is a novel machine learning setting which enables on-device intelligence via decentralized training and federated optimization. The rapid development of deep neural networks facilitates the learning techniques for modeling complex problems and emerges into federated deep learning under the federated setting. However, the tremendous amount of model parameters burdens the communication network with a high load of transportation. This paper introduces two approaches for improving communication efficiency by dynamic sampling and top-k selective masking. The former controls the fraction of selected client models dynamically, while the latter selects parameters with top-k largest values of difference for federated updating. Experiments on convolutional image classification and recurrent language modeling are conducted on three public datasets to show the effectiveness of our proposed methods.

<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](http://academicpages.github.io/files/paper1.pdf) -->

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->